{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-25T01:11:27.142470Z",
     "start_time": "2025-04-25T01:11:25.247622Z"
    }
   },
   "source": [
    "import os\n",
    "import re\n",
    "import chardet\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from collections import defaultdict\n",
    "\n",
    "############################\n",
    "# Configuration variables\n",
    "############################\n",
    "SOURCE_FILE = \"Data/posts.csv\"                 # Original CSV file (may not be UTF-8)\n",
    "UTF8_FILE = \"Data/posts_utf8.csv\"              # Transcoded UTF-8 CSV\n",
    "PREPROCESSED_FILE = \"Data/posts_preprocessed.csv\"\n",
    "EDGE_LIST_FILE = \"Data/weibo_cooccurrence_edges.csv\"\n",
    "\n",
    "STOPWORDS_FILES = [\n",
    "    \"Stopwords/baidu_stopwords.txt\",\n",
    "    \"Stopwords/cn_stopwords.txt\",\n",
    "    \"Stopwords/hit_stopwords.txt\",\n",
    "    \"Stopwords/scu_stopwords.txt\"\n",
    "]\n",
    "\n",
    "POST_COL_NAME = \"post\"                         # Column that stores raw text\n",
    "POST_TOKENS_COL_NAME = \"post_tokens\"           # New column that stores tokenized text\n",
    "WINDOW_SIZE = 5                                # Co-occurrence window size\n",
    "\n",
    "############################\n",
    "# Function section\n",
    "############################\n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    \"\"\"\n",
    "    Detect file encoding using chardet and return (encoding, confidence).\n",
    "    May return None if detection fails.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File does not exist: {file_path}\")\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = f.read()\n",
    "        result = chardet.detect(data)\n",
    "    encoding = result[\"encoding\"]\n",
    "    confidence = result[\"confidence\"]\n",
    "    print(f\"[INFO] Detected encoding by chardet: {encoding}, confidence: {confidence}\")\n",
    "    return encoding, confidence\n",
    "\n",
    "\n",
    "def convert_to_utf8(src_file, dst_file, src_encoding=None):\n",
    "    \"\"\"\n",
    "    Decode src_file with src_encoding, then write it out as UTF-8 to dst_file.\n",
    "    If src_encoding is None, default to gb18030.\n",
    "    \"\"\"\n",
    "    if src_encoding is None:\n",
    "        src_encoding = \"gb18030\"  # Broadest coverage for CJK encodings\n",
    "    try:\n",
    "        with open(src_file, \"rb\") as f_in:\n",
    "            data = f_in.read()\n",
    "        text = data.decode(src_encoding, errors=\"replace\")\n",
    "        with open(dst_file, \"w\", encoding=\"utf-8\") as f_out:\n",
    "            f_out.write(text)\n",
    "        print(f\"[INFO] Successfully transcoded '{src_file}' to UTF-8 => '{dst_file}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Transcoding failed: {e}\")\n",
    "\n",
    "\n",
    "def load_stopwords(paths):\n",
    "    \"\"\"\n",
    "    Merge multiple stopword files into one set and return it.\n",
    "    \"\"\"\n",
    "    sw = set()\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            with open(p, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "                for line in f:\n",
    "                    w = line.strip()\n",
    "                    if w:\n",
    "                        sw.add(w)\n",
    "        else:\n",
    "            print(f\"[WARNING] Stopword file not found: {p}\")\n",
    "    print(f\"[INFO] Stopwords loaded. Total: {len(sw)}\")\n",
    "    return sw\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text by:\n",
    "    1) Removing URLs\n",
    "    2) Removing @username mentions\n",
    "    3) Removing the outer #...# symbols while keeping the inner text\n",
    "       (If you want to delete the entire hashtag, use: text = re.sub(r'#[^#]*#', '', text))\n",
    "    4) Keeping only Chinese characters, English letters and digits;\n",
    "       all other symbols are replaced with spaces.\n",
    "    \"\"\"\n",
    "    # 1) Remove URLs\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    # 2) Remove @username\n",
    "    text = re.sub(r'@\\S+', '', text)\n",
    "    # 3) Strip hashtag shells but keep content\n",
    "    text = re.sub(r'#([^#]+)#', r'\\1', text)\n",
    "    # 4) Keep CJK, alphabetic, and numeric chars; replace others with spaces\n",
    "    text = re.sub(r'[^\\u4e00-\\u9fa5a-zA-Z0-9]+', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_text(text, stopwords):\n",
    "    \"\"\"\n",
    "    For a single post:\n",
    "    - clean_text\n",
    "    - jieba.lcut\n",
    "    - remove stopwords\n",
    "    - remove empty strings\n",
    "    Return a list of tokens.\n",
    "    \"\"\"\n",
    "    text = clean_text(text)\n",
    "    tokens = jieba.lcut(text)\n",
    "    tokens = [t for t in tokens if t.strip() and t not in stopwords]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def build_cooccurrence_edge_list(list_of_token_lists, window_size, outfile):\n",
    "    \"\"\"\n",
    "    Build a co-occurrence edge list from tokenized posts and write it to outfile (CSV).\n",
    "    Skips empty strings and writes only w1 < w2 to avoid duplicates.\n",
    "    \"\"\"\n",
    "    co_dict = defaultdict(lambda: defaultdict(int))\n",
    "    for tokens in list_of_token_lists:\n",
    "        if not tokens:                      # Skip posts with no valid tokens\n",
    "            continue\n",
    "        length = len(tokens)\n",
    "        for i in range(length):\n",
    "            w1 = tokens[i].strip()\n",
    "            if not w1:\n",
    "                continue\n",
    "            for j in range(i + 1, min(i + window_size, length)):\n",
    "                w2 = tokens[j].strip()\n",
    "                if w2 and w1 != w2:\n",
    "                    co_dict[w1][w2] += 1\n",
    "                    co_dict[w2][w1] += 1\n",
    "\n",
    "    # Write CSV\n",
    "    import csv\n",
    "    with open(outfile, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Source\", \"Target\", \"Weight\"])\n",
    "        for w1, neighbors in co_dict.items():\n",
    "            for w2, weight in neighbors.items():\n",
    "                if w1 < w2:                # Avoid duplicate edges\n",
    "                    writer.writerow([w1, w2, weight])\n",
    "\n",
    "    print(f\"[INFO] Edge list saved to: {outfile}\")\n",
    "\n",
    "\n",
    "############################\n",
    "# Main pipeline\n",
    "############################\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Detect encoding of the original file\n",
    "    if not os.path.exists(SOURCE_FILE):\n",
    "        print(f\"[ERROR] Source file not found: {SOURCE_FILE}\")\n",
    "        exit()\n",
    "\n",
    "    encoding, confidence = detect_encoding(SOURCE_FILE)\n",
    "\n",
    "    # 2. Transcode to UTF-8 (if not already UTF-8)\n",
    "    if encoding is None:\n",
    "        print(\"[WARNING] Encoding not detected. Defaulting to gb18030 for transcoding.\")\n",
    "        encoding = \"gb18030\"\n",
    "    elif encoding.lower() == \"utf-8\":\n",
    "        print(\"[INFO] File is already UTF-8. Creating a direct copy.\")\n",
    "        import shutil\n",
    "        shutil.copyfile(SOURCE_FILE, UTF8_FILE)\n",
    "    else:\n",
    "        print(f\"[INFO] File is likely {encoding}. Attempting gb18030 → UTF-8 transcoding...\")\n",
    "        convert_to_utf8(SOURCE_FILE, UTF8_FILE, src_encoding=\"gb18030\")\n",
    "\n",
    "    # 3. Load the UTF-8 file into pandas\n",
    "    try:\n",
    "        df = pd.read_csv(UTF8_FILE, encoding=\"utf-8\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Unable to read {UTF8_FILE} with UTF-8: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # 4. Verify the target column exists\n",
    "    if POST_COL_NAME not in df.columns:\n",
    "        print(f\"[ERROR] DataFrame does not contain column '{POST_COL_NAME}'. Please check column names.\")\n",
    "        print(f\"Available columns: {df.columns.tolist()}\")\n",
    "        exit()\n",
    "\n",
    "    # 5. Load stopwords\n",
    "    stopwords = load_stopwords(STOPWORDS_FILES)\n",
    "\n",
    "    # 6. Preprocess and tokenize each post\n",
    "    all_token_lists = []\n",
    "    for text in df[POST_COL_NAME]:\n",
    "        text = str(text)                   # Ensure string type\n",
    "        tokens = preprocess_text(text, stopwords)\n",
    "        all_token_lists.append(tokens)\n",
    "\n",
    "    # 7. Store tokenization results back to DataFrame\n",
    "    df[POST_TOKENS_COL_NAME] = [\" \".join(toks) for toks in all_token_lists]\n",
    "    df.to_csv(PREPROCESSED_FILE, index=False, encoding=\"utf-8\")\n",
    "    print(f\"[INFO] Preprocessing complete. Saved to: {PREPROCESSED_FILE}\")\n",
    "\n",
    "    # 8. Build co-occurrence edge list\n",
    "    build_cooccurrence_edge_list(all_token_lists, WINDOW_SIZE, EDGE_LIST_FILE)\n",
    "    print(\"[INFO] Pipeline finished!\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Detected encoding by chardet: GB2312, confidence: 0.99\n",
      "[INFO] File is likely GB2312. Attempting gb18030 → UTF-8 transcoding...\n",
      "[INFO] Successfully transcoded 'Data/posts.csv' to UTF-8 => 'Data/posts_utf8.csv'\n",
      "[INFO] Stopwords loaded. Total: 2323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache C:\\Users\\gyc\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.405 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Preprocessing complete. Saved to: Data/posts_preprocessed.csv\n",
      "[INFO] Edge list saved to: Data/weibo_cooccurrence_edges.csv\n",
      "[INFO] Pipeline finished!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T01:11:50.261133Z",
     "start_time": "2025-04-25T01:11:46.370317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "analyze_top100_edges.py\n",
    "────────────────────────────────────────────────────────────\n",
    "• Load an edge list   Source, Target, Weight   (CSV)\n",
    "• Get the 100 nodes with the highest total Weight (frequency)\n",
    "• Build the sub‑graph of those nodes\n",
    "      – compute PMI for every edge\n",
    "      – save the edge table sorted by PMI ↓\n",
    "• Compute three node‑level metrics on that sub‑graph\n",
    "      – degree_centrality     (un‑weighted)\n",
    "      – strength_weight       (weighted degree = Σ Weight)\n",
    "      – eigenvector_centrality   (edge weight = PMI)\n",
    "      – save the node table sorted by eigenvector_centrality ↓\n",
    "Outputs\n",
    "───────\n",
    "top100_freq.csv          word, freq\n",
    "top100_edges_pmi.csv     Source, Target, Weight, PMI     (sorted by PMI ↓)\n",
    "top100_centrality.csv    word, degree_centrality, strength_weight,\n",
    "                         eigenvector_centrality          (sorted by eigencent ↓)\n",
    "Dependencies :  pandas  networkx  numpy  (scipy optional but faster)\n",
    "pip install pandas networkx numpy scipy\n",
    "────────────────────────────────────────────────────────────\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from math import log2\n",
    "\n",
    "# ── configuration ─────────────────────────────────────────\n",
    "EDGE_FILE = \"Data/weibo_cooccurrence_edges.csv\"   # input CSV\n",
    "TOP_N     = 100                                   # how many top nodes\n",
    "OUT_FREQ  = \"top100_freq.csv\"\n",
    "OUT_EDGE  = \"top100_edges_pmi.csv\"\n",
    "OUT_CENT  = \"top100_centrality.csv\"\n",
    "# ──────────────────────────────────────────────────────────\n",
    "\n",
    "print(\"\\n[1] Loading edge list …\")\n",
    "df = pd.read_csv(EDGE_FILE, encoding=\"utf-8\")\n",
    "\n",
    "required_cols = {\"Source\", \"Target\", \"Weight\"}\n",
    "if not required_cols.issubset(df.columns):\n",
    "    raise KeyError(f\"CSV must contain columns: {required_cols}\")\n",
    "\n",
    "# ── 2. frequency (node strength) ──────────────────────────\n",
    "print(\"[2] Computing node frequencies …\")\n",
    "strength = {}\n",
    "for _, row in df.iterrows():\n",
    "    s, t, w = row[\"Source\"], row[\"Target\"], row[\"Weight\"]\n",
    "    strength[s] = strength.get(s, 0) + w\n",
    "    strength[t] = strength.get(t, 0) + w\n",
    "\n",
    "top_nodes = sorted(strength.items(), key=lambda x: x[1], reverse=True)[:TOP_N]\n",
    "pd.DataFrame(top_nodes, columns=[\"word\", \"freq\"])\\\n",
    "  .to_csv(OUT_FREQ, index=False, encoding=\"utf-8\")\n",
    "print(f\"    Top‑{TOP_N} freq table  →  {OUT_FREQ}\")\n",
    "\n",
    "top_set = {w for w, _ in top_nodes}\n",
    "\n",
    "# ── 3. build sub‑graph and compute PMI ────────────────────\n",
    "print(f\"[3] Building Top‑{TOP_N} sub‑graph …\")\n",
    "G = nx.Graph()\n",
    "for _, row in df.iterrows():\n",
    "    s, t, w = row[\"Source\"], row[\"Target\"], row[\"Weight\"]\n",
    "    if G.has_edge(s, t):\n",
    "        G[s][t][\"weight\"] += w          # accumulate duplicates\n",
    "    else:\n",
    "        G.add_edge(s, t, weight=w)\n",
    "\n",
    "H = G.subgraph(top_set).copy()\n",
    "print(f\"    Sub‑graph: {H.number_of_nodes()} nodes, {H.number_of_edges()} edges\")\n",
    "\n",
    "# total edge weight in sub‑graph\n",
    "T = sum(d[\"weight\"] for _, _, d in H.edges(data=True))\n",
    "# node frequencies restricted to sub‑graph\n",
    "f = {n: sum(d[\"weight\"] for _, _, d in H.edges(n, data=True)) for n in H.nodes}\n",
    "\n",
    "# compute PMI per edge\n",
    "for u, v, d in H.edges(data=True):\n",
    "    w_uv = d[\"weight\"]\n",
    "    pmi  = log2((w_uv * T) / (f[u] * f[v])) if w_uv > 0 else 0.0\n",
    "    d[\"pmi\"] = pmi\n",
    "\n",
    "# positive shift if negative PMI exists (for algorithms needing ≥0)\n",
    "min_pmi = min(d[\"pmi\"] for _, _, d in H.edges(data=True))\n",
    "if min_pmi < 0:\n",
    "    shift = abs(min_pmi) + 1e-6\n",
    "    for _, _, d in H.edges(data=True):\n",
    "        d[\"pmi_pos\"] = d[\"pmi\"] + shift\n",
    "    pmi_key = \"pmi_pos\"\n",
    "else:\n",
    "    pmi_key = \"pmi\"\n",
    "\n",
    "# save edge list sorted by PMI\n",
    "edges_sorted = sorted(\n",
    "    ((u, v, d[\"weight\"], d[\"pmi\"]) for u, v, d in H.edges(data=True)),\n",
    "    key=lambda x: x[3], reverse=True\n",
    ")\n",
    "pd.DataFrame(edges_sorted,\n",
    "             columns=[\"Source\", \"Target\", \"Weight\", \"PMI\"])\\\n",
    "  .to_csv(OUT_EDGE, index=False, encoding=\"utf-8\")\n",
    "print(f\"    Edge list + PMI  →  {OUT_EDGE}\")\n",
    "\n",
    "# ── 4. centrality metrics ─────────────────────────────────\n",
    "print(\"[4] Computing centrality metrics …\")\n",
    "deg_cent = nx.degree_centrality(H)\n",
    "strength_weight = {n: f[n] for n in H.nodes}\n",
    "\n",
    "# eigenvector centrality : prefer SciPy backend if available\n",
    "try:\n",
    "    import scipy  # noqa: F401\n",
    "    eig_cent = nx.eigenvector_centrality_numpy(H, weight=pmi_key)\n",
    "except ImportError:\n",
    "    eig_cent = nx.eigenvector_centrality(H, weight=pmi_key,\n",
    "                                         max_iter=500, tol=1e-6)\n",
    "\n",
    "rows = [{\n",
    "    \"word\": n,\n",
    "    \"degree_centrality\":       deg_cent[n],\n",
    "    \"strength_weight\":         strength_weight[n],\n",
    "    \"eigenvector_centrality\":  eig_cent[n]\n",
    "} for n in top_set]\n",
    "\n",
    "(pd.DataFrame(rows)\n",
    "     .sort_values(\"eigenvector_centrality\", ascending=False)\n",
    "     .to_csv(OUT_CENT, index=False, encoding=\"utf-8\"))\n",
    "print(f\"    Centrality table  →  {OUT_CENT}\\n✓ Done.\")\n"
   ],
   "id": "33138d204e7e8529",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] Loading edge list …\n",
      "[2] Computing node frequencies …\n",
      "    Top‑100 freq table  →  top100_freq.csv\n",
      "[3] Building Top‑100 sub‑graph …\n",
      "    Sub‑graph: 100 nodes, 2599 edges\n",
      "    Edge list + PMI  →  top100_edges_pmi.csv\n",
      "[4] Computing centrality metrics …\n",
      "    Centrality table  →  top100_centrality.csv\n",
      "✓ Done.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "404279761a490dc0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
